{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f44cca3-12f8-48c5-9af7-9f920ba103f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Udemy Course Scraper - Jupyter Notebook Version\n",
    "# Cell 1: Install and Import Dependencies\n",
    "# Run this cell first to install required packages\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    packages = ['requests', 'beautifulsoup4', 'pandas', 'openpyxl', 'lxml', 'ipywidgets', 'tqdm']\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace('-', '_'))\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Uncomment the line below if you need to install packages\n",
    "# install_packages()\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d959a832-423d-4a58-a0a6-f9f01f564fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… UdemyCoursesScraper class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define the UdemyCourseScraper Class\n",
    "class UdemyCoursesScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.courses_data = []\n",
    "        self.progress_bar = None\n",
    "        \n",
    "    def login(self, email, password):\n",
    "        \"\"\"Login to Udemy account with better error handling\"\"\"\n",
    "        print(\"ğŸ” Logging into Udemy...\")\n",
    "        \n",
    "        try:\n",
    "            # Get login page to extract CSRF token\n",
    "            login_url = \"https://www.udemy.com/join/login-popup/\"\n",
    "            response = self.session.get(login_url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find CSRF token\n",
    "            csrf_token = None\n",
    "            csrf_input = soup.find('input', {'name': 'csrfmiddlewaretoken'})\n",
    "            if csrf_input:\n",
    "                csrf_token = csrf_input.get('value')\n",
    "            \n",
    "            # Login data\n",
    "            login_data = {\n",
    "                'email': email,\n",
    "                'password': password,\n",
    "                'csrfmiddlewaretoken': csrf_token,\n",
    "            }\n",
    "            \n",
    "            # Perform login\n",
    "            login_response = self.session.post(\n",
    "                \"https://www.udemy.com/join/login-popup/\",\n",
    "                data=login_data,\n",
    "                headers={'Referer': login_url}\n",
    "            )\n",
    "            \n",
    "            # Check if login was successful\n",
    "            if login_response.status_code == 200:\n",
    "                # Verify by checking if we can access the dashboard\n",
    "                dashboard_response = self.session.get(\"https://www.udemy.com/home/my-courses/learning/\")\n",
    "                if dashboard_response.status_code == 200 and \"my-courses\" in dashboard_response.url:\n",
    "                    print(\"âœ… Login successful!\")\n",
    "                    return True\n",
    "            \n",
    "            print(\"âŒ Login failed. Please check your credentials.\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Login error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_my_courses(self):\n",
    "        \"\"\"Get list of purchased courses with progress tracking\"\"\"\n",
    "        print(\"ğŸ“š Fetching your courses...\")\n",
    "        \n",
    "        try:\n",
    "            my_courses_url = \"https://www.udemy.com/home/my-courses/learning/\"\n",
    "            response = self.session.get(my_courses_url)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(\"âŒ Failed to access courses page\")\n",
    "                return []\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for course cards with multiple strategies\n",
    "            course_links = []\n",
    "            \n",
    "            selectors = [\n",
    "                'a[href*=\"/course/\"]',\n",
    "                '.course-card-title a',\n",
    "                '.course-title a',\n",
    "                '[data-purpose=\"course-title-url\"]',\n",
    "                'a[data-purpose=\"course-card-title\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                links = soup.select(selector)\n",
    "                if links:\n",
    "                    course_links.extend([link.get('href') for link in links if link.get('href')])\n",
    "                    break\n",
    "            \n",
    "            # Clean and deduplicate course URLs\n",
    "            course_urls = set()\n",
    "            for link in course_links:\n",
    "                if link:\n",
    "                    if link.startswith('/'):\n",
    "                        full_url = urljoin(\"https://www.udemy.com\", link)\n",
    "                    else:\n",
    "                        full_url = link\n",
    "                    \n",
    "                    if '/course/' in full_url:\n",
    "                        course_urls.add(full_url)\n",
    "            \n",
    "            course_urls = list(course_urls)\n",
    "            print(f\"âœ… Found {len(course_urls)} courses\")\n",
    "            return course_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fetching courses: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_course_details(self, course_url):\n",
    "        \"\"\"Scrape details for a single course with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(course_url)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract course title\n",
    "            title_selectors = [\n",
    "                '[data-purpose=\"course-header-title\"]',\n",
    "                'h1[class*=\"course-title\"]',\n",
    "                'h1.clp-lead__title',\n",
    "                'h1'\n",
    "            ]\n",
    "            title = self.find_text_by_selectors(soup, title_selectors, \"Title not found\")\n",
    "            \n",
    "            # Extract description\n",
    "            desc_selectors = [\n",
    "                '[data-purpose=\"course-description\"]',\n",
    "                '.course-description',\n",
    "                '[class*=\"description\"]',\n",
    "                '.clp-lead__headline'\n",
    "            ]\n",
    "            description = self.find_text_by_selectors(soup, desc_selectors, \"Description not found\")\n",
    "            \n",
    "            # Extract instructor\n",
    "            instructor_selectors = [\n",
    "                '[data-purpose=\"instructor-name\"]',\n",
    "                '.instructor-name',\n",
    "                '[class*=\"instructor\"] a',\n",
    "                '.clp-lead__instructor-name a',\n",
    "                'a[href*=\"/user/\"]'\n",
    "            ]\n",
    "            instructor = self.find_text_by_selectors(soup, instructor_selectors, \"Instructor not found\")\n",
    "            \n",
    "            # Extract number of lectures\n",
    "            lectures_selectors = [\n",
    "                '[data-purpose=\"curriculum-stats-lectures\"]',\n",
    "                '[class*=\"lectures\"]',\n",
    "                'span:contains(\"lectures\")',\n",
    "                'span:contains(\"lecture\")'\n",
    "            ]\n",
    "            lectures_text = self.find_text_by_selectors(soup, lectures_selectors, \"0\")\n",
    "            lectures = self.extract_number(lectures_text)\n",
    "            \n",
    "            # Extract total time\n",
    "            time_selectors = [\n",
    "                '[data-purpose=\"curriculum-stats-duration\"]',\n",
    "                '[class*=\"duration\"]',\n",
    "                'span:contains(\"total\")',\n",
    "                'span:contains(\"hours\")',\n",
    "                'span:contains(\"hour\")'\n",
    "            ]\n",
    "            duration_text = self.find_text_by_selectors(soup, time_selectors, \"Duration not found\")\n",
    "            \n",
    "            # Clean up the course URL to remove query parameters\n",
    "            clean_url = course_url.split('?')[0]\n",
    "            \n",
    "            course_data = {\n",
    "                'Title': title,\n",
    "                'Description': description[:500] + \"...\" if len(description) > 500 else description,\n",
    "                'Instructor': instructor,\n",
    "                'Number of Lectures': lectures,\n",
    "                'Total Duration': duration_text,\n",
    "                'Course URL': clean_url\n",
    "            }\n",
    "            \n",
    "            return course_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error scraping course {course_url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def find_text_by_selectors(self, soup, selectors, default=\"\"):\n",
    "        \"\"\"Try multiple selectors to find text content\"\"\"\n",
    "        for selector in selectors:\n",
    "            if ':contains(' in selector:\n",
    "                text_to_find = selector.split(':contains(')[1].rstrip(')')\n",
    "                elements = soup.find_all(text=re.compile(text_to_find, re.I))\n",
    "                if elements:\n",
    "                    parent = elements[0].parent\n",
    "                    return parent.get_text(strip=True) if parent else default\n",
    "            else:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    return element.get_text(strip=True)\n",
    "        return default\n",
    "    \n",
    "    def extract_number(self, text):\n",
    "        \"\"\"Extract number from text\"\"\"\n",
    "        numbers = re.findall(r'\\d+', text)\n",
    "        return int(numbers[0]) if numbers else 0\n",
    "    \n",
    "    def scrape_all_courses(self, delay=2):\n",
    "        \"\"\"Scrape all purchased courses with progress bar\"\"\"\n",
    "        course_urls = self.get_my_courses()\n",
    "        \n",
    "        if not course_urls:\n",
    "            print(\"âŒ No courses found. Make sure you're logged in correctly.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        self.courses_data = []\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        for url in tqdm(course_urls, desc=\"Scraping courses\"):\n",
    "            course_data = self.scrape_course_details(url)\n",
    "            \n",
    "            if course_data:\n",
    "                self.courses_data.append(course_data)\n",
    "            \n",
    "            # Be respectful - add delay between requests\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        df = pd.DataFrame(self.courses_data)\n",
    "        print(f\"âœ… Completed! Scraped {len(self.courses_data)} courses successfully.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_to_excel(self, df, filename=\"udemy_courses.xlsx\"):\n",
    "        \"\"\"Export dataframe to Excel with formatting\"\"\"\n",
    "        if df.empty:\n",
    "            print(\"âŒ No course data to export.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "                df.to_excel(writer, sheet_name='My Udemy Courses', index=False)\n",
    "                \n",
    "                # Get workbook and worksheet\n",
    "                workbook = writer.book\n",
    "                worksheet = writer.sheets['My Udemy Courses']\n",
    "                \n",
    "                # Adjust column widths\n",
    "                for column in worksheet.columns:\n",
    "                    max_length = 0\n",
    "                    column_letter = column[0].column_letter\n",
    "                    \n",
    "                    for cell in column:\n",
    "                        try:\n",
    "                            if len(str(cell.value)) > max_length:\n",
    "                                max_length = len(str(cell.value))\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    adjusted_width = min(max_length + 2, 50)\n",
    "                    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "            \n",
    "            print(f\"âœ… Data exported to {filename}\")\n",
    "            print(f\"ğŸ“Š Total courses exported: {len(df)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error exporting to Excel: {str(e)}\")\n",
    "\n",
    "print(\"âœ… UdemyCoursesScraper class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae10922-09bf-4b04-877a-7a31f5509929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Login and Authentication\n",
    "def login_to_udemy():\n",
    "    \"\"\"Interactive login function for Jupyter\"\"\"\n",
    "    scraper = UdemyCoursesScraper()\n",
    "    \n",
    "    print(\"ğŸ“ Udemy Course Scraper\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Use getpass for secure password input\n",
    "    email = input(\"Enter your Udemy email: \")\n",
    "    password = getpass.getpass(\"Enter your Udemy password: \")\n",
    "    \n",
    "    success = scraper.login(email, password)\n",
    "    \n",
    "    if success:\n",
    "        return scraper\n",
    "    else:\n",
    "        print(\"Please try again with correct credentials.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8b324b-56f4-4aec-ae78-f7f091e7048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Scraping Function\n",
    "def scrape_and_display_courses(scraper, delay=2, export_excel=True, filename=\"udemy_courses.xlsx\"):\n",
    "    \"\"\"Main function to scrape courses and display results\"\"\"\n",
    "    if scraper is None:\n",
    "        print(\"âŒ Please login first using the login_to_udemy() function\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸš€ Starting course scraping...\")\n",
    "    \n",
    "    # Scrape all courses\n",
    "    df = scraper.scrape_all_courses(delay=delay)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âŒ No courses were scraped successfully.\")\n",
    "        return None\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"Total courses: {len(df)}\")\n",
    "    print(f\"Total lectures: {df['Number of Lectures'].sum()}\")\n",
    "    print(f\"Unique instructors: {df['Instructor'].nunique()}\")\n",
    "    \n",
    "    # Display first few courses\n",
    "    print(f\"\\nğŸ“‹ Preview of your courses:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Export to Excel if requested\n",
    "    if export_excel:\n",
    "        scraper.export_to_excel(df, filename)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1a6640-5d65-4d77-b98e-d1c6332ebb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ How to use this scraper in Jupyter:\n",
      "\n",
      "1. Run Cell 1 to install dependencies\n",
      "2. Run Cell 2 to define the scraper class  \n",
      "3. Run Cell 3 to login:\n",
      "   scraper = login_to_udemy()\n",
      "\n",
      "4. Run Cell 4 to scrape your courses:\n",
      "   df = scrape_and_display_courses(scraper)\n",
      "\n",
      "5. Optional: Analyze your data further:\n",
      "   # Group by instructor\n",
      "   instructor_stats = df.groupby('Instructor').agg({\n",
      "       'Title': 'count',\n",
      "       'Number of Lectures': 'sum'\n",
      "   }).rename(columns={'Title': 'Course Count'})\n",
      "   \n",
      "   display(instructor_stats.sort_values('Course Count', ascending=False))\n",
      "\n",
      "âš™ï¸ Customization options:\n",
      "- Change delay between requests: scrape_and_display_courses(scraper, delay=3)\n",
      "- Custom filename: scrape_and_display_courses(scraper, filename=\"my_courses.xlsx\")\n",
      "- Skip Excel export: scrape_and_display_courses(scraper, export_excel=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Usage Instructions\n",
    "print(\"\"\"\n",
    "ğŸ¯ How to use this scraper in Jupyter:\n",
    "\n",
    "1. Run Cell 1 to install dependencies\n",
    "2. Run Cell 2 to define the scraper class  \n",
    "3. Run Cell 3 to login:\n",
    "   scraper = login_to_udemy()\n",
    "\n",
    "4. Run Cell 4 to scrape your courses:\n",
    "   df = scrape_and_display_courses(scraper)\n",
    "\n",
    "5. Optional: Analyze your data further:\n",
    "   # Group by instructor\n",
    "   instructor_stats = df.groupby('Instructor').agg({\n",
    "       'Title': 'count',\n",
    "       'Number of Lectures': 'sum'\n",
    "   }).rename(columns={'Title': 'Course Count'})\n",
    "   \n",
    "   display(instructor_stats.sort_values('Course Count', ascending=False))\n",
    "\n",
    "âš™ï¸ Customization options:\n",
    "- Change delay between requests: scrape_and_display_courses(scraper, delay=3)\n",
    "- Custom filename: scrape_and_display_courses(scraper, filename=\"my_courses.xlsx\")\n",
    "- Skip Excel export: scrape_and_display_courses(scraper, export_excel=False)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2eee44-a951-490b-b2a7-b45a762270c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Udemy Course Scraper\n",
      "==============================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Udemy email:  jarrettkw@gmail.com\n",
      "Enter your Udemy password:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Logging into Udemy...\n",
      "âŒ Login failed. Please check your credentials.\n",
      "Please try again with correct credentials.\n"
     ]
    }
   ],
   "source": [
    "scraper = login_to_udemy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63b14b-27d8-4b4c-8b07-2febb0956b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
